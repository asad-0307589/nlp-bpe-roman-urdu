{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b7a728f-bae8-4980-a412-1e65735d8e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training & Testing Files Loaded Successfully!\n",
      "Total Training Sentences: 1018\n",
      "Total Testing Sentences: 247\n",
      "\n",
      " Sample Training Data: ['1. subha 5 bjhey uthna perha trip thaa jaldi jaldi ready hua aur 0540 ghar saay nikal gaay\\n', '2. 0615 bus chal perhee joo kaay first time thaa kaay trip time saay chala\\n', '3. nust kaay 3 larkay thy unsaay batain kee phir bluetooth trip coordinator kaay pass thaa tou humm shoor daltay rahay kaay song change krr dain\\n']\n",
      "\n",
      " Sample Testing Data: ['1. Subah utha, brush kiya aur naha dho kar nashta kiya.\\n', '2. Ghar ke kaam kiye aur sabzi lene bazar gaya.\\n', '3. Sabzi lekar aya aur khane ki tayari mein madad ki.\\n']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Folder path\n",
    "train_folder = r\"C:\\Users\\ASAD SHAH\\Desktop\\NLP-Assignment-22i-0597\\BPE_dataset_Training\\dataset\"\n",
    "test_folder = r\"C:\\Users\\ASAD SHAH\\Desktop\\NLP-Assignment-22i-0597\\BPE_dataset_Testing\"\n",
    "\n",
    "def loaded_text(folder_path):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"): \n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data.extend(f.readlines())  # Read each line\n",
    "            except UnicodeDecodeError:\n",
    "                with open(file_path, \"r\", encoding=\"ISO-8859-1\") as f: \n",
    "                    data.extend(f.readlines())\n",
    "    return data\n",
    "\n",
    "# Load training and testing datasets\n",
    "train_data = loaded_text(train_folder)\n",
    "test_data = loaded_text(test_folder)\n",
    "\n",
    "print(\" Training & Testing Files Loaded Successfully!\")\n",
    "print(f\"Total Training Sentences: {len(train_data)}\")\n",
    "print(f\"Total Testing Sentences: {len(test_data)}\")\n",
    "\n",
    "\n",
    "print(\"\\n Sample Training Data:\", train_data[:3])\n",
    "print(\"\\n Sample Testing Data:\", test_data[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af26b87b-32a7-4f2b-af36-4f58a38cfeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Text Preprocessing Completed!\n",
      "\n",
      " Sample Cleaned Training Data: ['1 subha 5 bjhey uthna perha trip thaa jaldi jaldi ready hua aur 0540 ghar saay nikal gaay', '2 0615 bus chal perhee joo kaay first time thaa kaay trip time saay chala', '3 nust kaay 3 larkay thy unsaay batain kee phir bluetooth trip coordinator kaay pass thaa tou humm shoor daltay rahay kaay song change krr dain']\n",
      "\n",
      " Sample Cleaned Testing Data: ['1 subah utha brush kiya aur naha dho kar nashta kiya', '2 ghar ke kaam kiye aur sabzi lene bazar gaya', '3 sabzi lekar aya aur khane ki tayari mein madad ki']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() \n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces\n",
    "    return text\n",
    "\n",
    "train_data_cleaned = [preprocess_text(sentence) for sentence in train_data if sentence.strip()]\n",
    "test_data_cleaned = [preprocess_text(sentence) for sentence in test_data if sentence.strip()]\n",
    "\n",
    "\n",
    "print(\" Text Preprocessing Completed!\")\n",
    "print(\"\\n Sample Cleaned Training Data:\", train_data_cleaned[:3])\n",
    "print(\"\\n Sample Cleaned Testing Data:\", test_data_cleaned[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabc12de-fa72-4138-8ee8-3fec8c490de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BPE Model Loaded Successfully!\n",
      "Total Vocabulary Size: 2619\n",
      "Total Merge Rules: 2077\n",
      "ðŸ“Œ First 20 Merge Rules: [('h', 'a'), ('a', 'y'), ('k', 'a'), ('h', 'i'), ('k', 'i'), ('b', 'a'), ('i', 'n'), ('u', 'r'), ('m', 'a'), ('n', 'a'), ('m', 'e'), ('h', 'o'), ('a', 'r'), ('l', 'a'), ('k', 'e'), ('a', 'ur'), ('n', 'e'), ('y', 'a'), ('a', 'a'), ('s', 'e')]\n",
      "ðŸ“Œ Total Merge Rules: 2077\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the BPE model from pickle file\n",
    "with open(\"bpe_model.pkl\", \"rb\") as f:\n",
    "    bpe_model = pickle.load(f)\n",
    "\n",
    "# Extract vocabulary and merges\n",
    "vocab = bpe_model[\"vocab\"]\n",
    "merges = bpe_model[\"merges\"]\n",
    "\n",
    "print(\" BPE Model Loaded Successfully!\")\n",
    "print(f\"Total Vocabulary Size: {len(vocab)}\")\n",
    "print(f\"Total Merge Rules: {len(merges)}\")\n",
    "print(\"ðŸ“Œ First 20 Merge Rules:\", merges[:20])  # Show first 20 merges\n",
    "print(\"ðŸ“Œ Total Merge Rules:\", len(merges))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57014638-cc09-4519-82df-db03af75f978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  No more pairs to merge. Current vocab size: 2619\n",
      "\n",
      "âœ… BPE Model Trained & Saved Successfully!\n",
      "Final Vocabulary Size: 1000 (Target: 1000)\n",
      "Total Merge Rules: 2077\n",
      "Total Tokens with IDs: 1000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def get_vocab(data):\n",
    "    vocab = Counter()\n",
    "    for sentence in data:\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            vocab[\" \".join(word)] += 1 \n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab(train_data_cleaned)\n",
    "\n",
    "# Set vocabulary size limit\n",
    "vocab_size = 1000  \n",
    "merges = [] \n",
    "\n",
    "\n",
    "while len(vocab) > vocab_size:  \n",
    "    pairs = Counter()\n",
    "\n",
    "    # Count all adjacent character pairs\n",
    "    for word in vocab:\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += vocab[word]\n",
    "\n",
    "    if not pairs:\n",
    "        print(f\"âš  No more pairs to merge. Current vocab size: {len(vocab)}\")\n",
    "        break  \n",
    "\n",
    "\n",
    "    most_common_pair = max(pairs, key=pairs.get)\n",
    "    merges.append(most_common_pair)\n",
    "\n",
    "    # Merge the most common pair in vocabulary\n",
    "    new_vocab = {}\n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(\" \".join(most_common_pair), \"\".join(most_common_pair))\n",
    "        new_vocab[new_word] = freq\n",
    "    vocab = new_vocab  # Update vocabulary\n",
    "\n",
    "#  **Fix: Ensure Vocabulary Size is Exactly 1000**\n",
    "if len(vocab) > vocab_size:\n",
    "    vocab = dict(list(vocab.items())[:vocab_size])  # Keep only 1000 subwords\n",
    "\n",
    "final_vocab = list(vocab.keys())  # Store only subwords\n",
    "\n",
    "# Assign unique IDs to each subword\n",
    "token_to_id = {token: idx for idx, token in enumerate(final_vocab, start=1)}  # Unique IDs\n",
    "id_to_token = {idx: token for token, idx in token_to_id.items()}  \n",
    "\n",
    "# Save Updated BPE Model (Final Vocabulary + Merges + ID Mappings)\n",
    "bpe_model = {\n",
    "    \"vocab\": final_vocab,  \n",
    "    \"merges\": merges, \n",
    "    \"token_to_id\": token_to_id,  \n",
    "    \"id_to_token\": id_to_token\n",
    "}\n",
    "\n",
    "with open(\"bpe_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bpe_model, f)\n",
    "\n",
    "print(\"\\n BPE Model Trained & Saved Successfully!\")\n",
    "print(f\"Final Vocabulary Size: {len(final_vocab)} (Target: 1000)\")\n",
    "print(f\"Total Merge Rules: {len(merges)}\")\n",
    "print(f\"Total Tokens with IDs: {len(token_to_id)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1b4cabc8-beff-46a0-bc93-fa54238b7e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BPE Model Loaded Successfully!\n",
      "Total Tokens: 1000 (Target: 1000)\n",
      "Total Merge Rules: 2077\n",
      "\n",
      " Sample Token-to-ID Mapping:\n",
      "1 â†’ 1\n",
      "subha â†’ 2\n",
      "5 â†’ 3\n",
      "bjhey â†’ 4\n",
      "uthna â†’ 5\n",
      "perha â†’ 6\n",
      "trip â†’ 7\n",
      "thaa â†’ 8\n",
      "jaldi â†’ 9\n",
      "ready â†’ 10\n",
      "hua â†’ 11\n",
      "aur â†’ 12\n",
      "0540 â†’ 13\n",
      "ghar â†’ 14\n",
      "saay â†’ 15\n",
      "nikal â†’ 16\n",
      "gaay â†’ 17\n",
      "2 â†’ 18\n",
      "0615 â†’ 19\n",
      "bus â†’ 20\n",
      "chal â†’ 21\n",
      "perhee â†’ 22\n",
      "joo â†’ 23\n",
      "kaay â†’ 24\n",
      "first â†’ 25\n",
      "time â†’ 26\n",
      "chala â†’ 27\n",
      "3 â†’ 28\n",
      "nust â†’ 29\n",
      "larkay â†’ 30\n",
      "thy â†’ 31\n",
      "unsaay â†’ 32\n",
      "batain â†’ 33\n",
      "kee â†’ 34\n",
      "phir â†’ 35\n",
      "bluetooth â†’ 36\n",
      "coordinator â†’ 37\n",
      "pass â†’ 38\n",
      "tou â†’ 39\n",
      "humm â†’ 40\n",
      "shoor â†’ 41\n",
      "daltay â†’ 42\n",
      "rahay â†’ 43\n",
      "song â†’ 44\n",
      "change â†’ 45\n",
      "krr â†’ 46\n",
      "dain â†’ 47\n",
      "4 â†’ 48\n",
      "murree â†’ 49\n",
      "mein â†’ 50\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the trained BPE model\n",
    "with open(\"bpe_model.pkl\", \"rb\") as f:\n",
    "    bpe_model = pickle.load(f)\n",
    "\n",
    "vocab = bpe_model[\"vocab\"]  \n",
    "merges = bpe_model[\"merges\"]  \n",
    "token_to_id = bpe_model[\"token_to_id\"]  \n",
    "id_to_token = bpe_model[\"id_to_token\"]  \n",
    "\n",
    "print(\"\\n BPE Model Loaded Successfully!\")\n",
    "print(f\"Total Tokens: {len(token_to_id)} (Target: 1000)\")\n",
    "print(f\"Total Merge Rules: {len(merges)}\")\n",
    "# Print first 50 token-to-ID mappings to check correctness\n",
    "print(\"\\n Sample Token-to-ID Mapping:\")\n",
    "for i, (token, token_id) in enumerate(token_to_id.items()):\n",
    "    print(f\"{token} â†’ {token_id}\")\n",
    "    if i == 49:  # Show only 50 samples\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7542e63-6fe5-428b-8e19-e44d7b645f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing Data Encoded Successfully!\n",
      " Sample Encoded Testing Data: [[1, 0, 0, 0, 0, 0, 187, 407, 0, 930, 0, 0, 0, 407, 0, 0, 0, 407, 0, 128, 0, 508, 0, 0, 0, 407, 0], [18, 0, 0, 117, 0, 365, 128, 656, 0, 0, 0, 0, 372, 0, 492, 179, 0, 0, 0, 809, 0], [28, 0, 0, 534, 0, 492, 128, 0, 0, 407, 0, 0, 0, 0, 0, 148, 0, 809, 0, 0, 0, 686, 0, 0, 148, 0]]\n"
     ]
    }
   ],
   "source": [
    "def encode(text, merges, token_to_id):\n",
    "    words = text.split()\n",
    "    encoded_ids = []\n",
    "\n",
    "    for word in words:\n",
    "        word_tokens = list(word)  \n",
    "\n",
    "        while True:\n",
    "            pairs = [(word_tokens[i], word_tokens[i+1]) for i in range(len(word_tokens)-1)]\n",
    "            merge_found = None\n",
    "\n",
    "            for pair in pairs:\n",
    "                if pair in merges:\n",
    "                    merge_found = pair\n",
    "                    break\n",
    "\n",
    "            if not merge_found:\n",
    "                break  # Stop if no more merges apply\n",
    "\n",
    "            # Apply the merge everywhere in the word\n",
    "            new_word_tokens = []\n",
    "            i = 0\n",
    "            while i < len(word_tokens):\n",
    "                if i < len(word_tokens) - 1 and (word_tokens[i], word_tokens[i+1]) == merge_found:\n",
    "                    new_word_tokens.append(\"\".join(merge_found))  # Merge pair\n",
    "                    i += 2  # Skip merged pair\n",
    "                else:\n",
    "                    new_word_tokens.append(word_tokens[i])\n",
    "                    i += 1\n",
    "            word_tokens = new_word_tokens\n",
    "\n",
    "        # Convert subwords to token IDs\n",
    "        word_ids = [token_to_id.get(token, 0) for token in word_tokens]  # 0 for unknown tokens\n",
    "        encoded_ids.extend(word_ids)\n",
    "\n",
    "    return encoded_ids\n",
    "\n",
    "# Encode testing dataset\n",
    "encoded_test_data = [encode(sentence, merges, token_to_id) for sentence in test_data]\n",
    "\n",
    "print(\"\\n Testing Data Encoded Successfully!\")\n",
    "print(\" Sample Encoded Testing Data:\", encoded_test_data[:3])  # Print first 3 encoded samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b32ad8c6-184e-46fd-a63a-72d595938992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Decoded Sentence: <UNK> a <UNK> uth a b <UNK> <UNK> <UNK> a\n"
     ]
    }
   ],
   "source": [
    "def decode(encoded_ids, id_to_token):\n",
    "    decoded_tokens = [id_to_token.get(token_id, \"<UNK>\") for token_id in encoded_ids]\n",
    "    return \" \".join(decoded_tokens)\n",
    "\n",
    "# Test decoding\n",
    "decoded_sentence = decode(encoded_sentence, id_to_token)\n",
    "\n",
    "print(\"\\n Decoded Sentence:\", decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c624f15-9b8f-4867-95ca-4043bf9d368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training & Testing Data Encoded Successfully!\n",
      " Sample Encoded Training Data: [[1, 0, 361, 407, 3, 4, 187, 508, 114, 555, 7, 0, 88, 9, 9, 10, 11, 0, 0, 13, 0, 0, 286, 0, 0, 0, 0, 0], [18, 0, 19, 0, 0, 0, 0, 114, 548, 0, 344, 222, 128, 0, 0, 0, 0, 26, 0, 88, 128, 0, 7, 26, 286, 0, 0, 0, 407], [28, 0, 0, 0, 128, 0, 28, 0, 0, 128, 0, 0, 0, 879, 286, 0, 0, 0, 0, 34, 0, 0, 0, 0, 0, 0, 7, 0, 190, 461, 508, 0, 190, 128, 0, 711, 0, 0, 0, 88, 39, 40, 0, 0, 0, 0, 0, 0, 0, 264, 0, 128, 0, 360, 0, 0, 0, 0, 0, 46, 0, 0]]\n",
      " Sample Encoded Testing Data: [[1, 0, 0, 0, 0, 0, 187, 407, 0, 930, 0, 0, 0, 407, 0, 0, 0, 407, 0, 128, 0, 508, 0, 0, 0, 407, 0], [18, 0, 0, 117, 0, 365, 128, 656, 0, 0, 0, 0, 372, 0, 492, 179, 0, 0, 0, 809, 0], [28, 0, 0, 534, 0, 492, 128, 0, 0, 407, 0, 0, 0, 0, 0, 148, 0, 809, 0, 0, 0, 686, 0, 0, 148, 0]]\n"
     ]
    }
   ],
   "source": [
    "def encode(text, merges, token_to_id):\n",
    "    words = text.split()\n",
    "    encoded_ids = []\n",
    "\n",
    "    for word in words:\n",
    "        word_tokens = list(word)  # Start with individual characters\n",
    "\n",
    "        while True:\n",
    "            pairs = [(word_tokens[i], word_tokens[i+1]) for i in range(len(word_tokens)-1)]\n",
    "            merge_found = None\n",
    "\n",
    "            for pair in pairs:\n",
    "                if pair in merges:\n",
    "                    merge_found = pair\n",
    "                    break\n",
    "\n",
    "            if not merge_found:\n",
    "                break  # Stop if no more merges apply\n",
    "\n",
    "            # Apply the merge everywhere in the word\n",
    "            new_word_tokens = []\n",
    "            i = 0\n",
    "            while i < len(word_tokens):\n",
    "                if i < len(word_tokens) - 1 and (word_tokens[i], word_tokens[i+1]) == merge_found:\n",
    "                    new_word_tokens.append(\"\".join(merge_found))  # Merge pair\n",
    "                    i += 2  # Skip merged pair\n",
    "                else:\n",
    "                    new_word_tokens.append(word_tokens[i])\n",
    "                    i += 1\n",
    "            word_tokens = new_word_tokens\n",
    "\n",
    "        # Convert subwords to token IDs\n",
    "        word_ids = [token_to_id.get(token, 0) for token in word_tokens]  # 0 for unknown tokens\n",
    "        encoded_ids.extend(word_ids)\n",
    "\n",
    "    return encoded_ids\n",
    "\n",
    "encoded_train_data = [encode(sentence, merges, token_to_id) for sentence in train_data]\n",
    "encoded_test_data = [encode(sentence, merges, token_to_id) for sentence in test_data]\n",
    "\n",
    "print(\"\\n Training & Testing Data Encoded Successfully!\")\n",
    "print(\" Sample Encoded Training Data:\", encoded_train_data[:3]) \n",
    "print(\" Sample Encoded Testing Data:\", encoded_test_data[:3]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a725e0e8-12b0-4ce6-a152-cae05f28c710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing Data Decoded Successfully!\n",
      " Sample Decoded Testing Data: ['1 <UNK> <UNK> <UNK> <UNK> <UNK> uth a <UNK> b <UNK> <UNK> <UNK> a <UNK> <UNK> <UNK> a <UNK> ka <UNK> na <UNK> <UNK> <UNK> a <UNK>', '2 <UNK> <UNK> ha <UNK> ke ka am <UNK> <UNK> <UNK> <UNK> sab <UNK> le ne <UNK> <UNK> <UNK> ya <UNK>', '3 <UNK> <UNK> ab <UNK> le ka <UNK> <UNK> a <UNK> <UNK> <UNK> <UNK> <UNK> ki <UNK> ya <UNK> <UNK> <UNK> ma <UNK> <UNK> ki <UNK>']\n"
     ]
    }
   ],
   "source": [
    "def decode(encoded_ids, id_to_token):\n",
    "    decoded_tokens = [id_to_token.get(token_id, \"<UNK>\") for token_id in encoded_ids]\n",
    "    return \" \".join(decoded_tokens)  # Convert tokens back to text\n",
    "    \n",
    "# Decode the encoded test dataset\n",
    "decoded_test_data = [decode(sentence, id_to_token) for sentence in encoded_test_data]\n",
    "\n",
    "print(\"\\n Testing Data Decoded Successfully!\")\n",
    "print(\" Sample Decoded Testing Data:\", decoded_test_data[:3])  # Print first 3 decoded samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f898b0b-e701-4a15-9904-d256edfe3bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **Vocabulary Reduction Evaluation**\n",
      " Original Vocabulary Size (before BPE): 2619\n",
      " Reduced Vocabulary Size (after BPE): 1000\n",
      " Reduction Percentage: 61.82%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_original_vocab_size(data):\n",
    "    vocab = set()\n",
    "    for sentence in data:\n",
    "        words = sentence.split()\n",
    "        vocab.update(words)  \n",
    "    return len(vocab)\n",
    "\n",
    "original_vocab_size = get_original_vocab_size(train_data_cleaned)\n",
    "\n",
    "\n",
    "bpe_vocab_size = len(token_to_id)  \n",
    "\n",
    "# Calculate reduction percentage\n",
    "vocab_reduction = ((original_vocab_size - bpe_vocab_size) / original_vocab_size) * 100\n",
    "\n",
    "print(\"\\n **Vocabulary Reduction Evaluation**\")\n",
    "print(f\" Original Vocabulary Size (before BPE): {original_vocab_size}\")\n",
    "print(f\" Reduced Vocabulary Size (after BPE): {bpe_vocab_size}\")\n",
    "print(f\" Reduction Percentage: {vocab_reduction:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6d82d-dbd2-4402-b013-d3f3b905bf18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
